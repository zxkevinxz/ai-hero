diff --git a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/arsenal.eval.ts b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/arsenal.eval.ts
index 477a8f8..62d5b4c 100644
--- a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/arsenal.eval.ts
+++ b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/arsenal.eval.ts
@@ -122,7 +122,7 @@ Export: Add the job to the render queue and render. DaVinci Resolve will create
       {
         id: "1",
         role: "user",
-        content: input,
+        parts: [{ type: "text", text: input }],
       },
     ];
     return askDeepSearch(messages);
@@ -140,4 +140,4 @@ Export: Add the job to the render queue and render. DaVinci Resolve will create
     },
     Factuality,
   ],
-});
\ No newline at end of file
+});
diff --git a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/initial.eval.ts b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/initial.eval.ts
index 4464f66..bcc4267 100644
--- a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/initial.eval.ts
+++ b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/evals/initial.eval.ts
@@ -37,7 +37,7 @@ Development and Build Performance: Improved build times and Faster Fast Refresh.
       {
         id: "1",
         role: "user",
-        content: input,
+        parts: [{ type: "text", text: input }],
       },
     ];
     return askDeepSearch(messages);
diff --git a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/src/server/scraper.ts b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/src/server/scraper.ts
index c06bda1..69530c4 100644
--- a/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/src/server/scraper.ts
+++ b/courses/01-deepsearch-in-typescript/00-apps/05-day-8-app/src/server/scraper.ts
@@ -96,7 +96,7 @@ const checkRobotsTxt = async (url: string): Promise<boolean> => {
       return true;
     }
 
-    const robotsTxt = await response.text.text();
+    const robotsTxt = await response.text();
     const robots = robotsParser(robotsUrl, robotsTxt);
 
     // Use a common crawler user agent
@@ -161,7 +161,7 @@ export const crawlWebsite = cacheWithRedis(
         const response = await fetch(url);
 
         if (response.ok) {
-          const html = await response.text.text();
+          const html = await response.text();
           const articleText = extractArticleText(html);
           return {
             success: true,
