---
id: lesson-q2yqx
---

In the last lesson we learned about the data flywheel - the virtuous loop that happens when you have users using your application in production.

The obvious lesson of the data flywheel is to launch fast. Get users using your application and generating data as soon as possible.

However, many applications or companies won't have that luxury. There are several situations where you'll need to build a dataset before launch:

- Stakeholders won't give you the green light until you can prove the app works.
- You expect a LOT of usage on day one, and want to test that adversarial users can't break the app.
- The application is in a quality-sensitive industry, like finance or healthcare. It needs to work well on day one.

This means that you will need to know how to construct datasets from scratch.

### What does a good evaluation dataset look like?

Data sets often come in three main sizes:

| Type       | Size   | Frequency          | Purpose                                                          |
| ---------- | ------ | ------------------ | ---------------------------------------------------------------- |
| Dev        | 10-20  | During development | Local testing of toughest cases                                  |
| CI         | 50-200 | Before deployment  | Pre-deployment testing                                           |
| Regression | 500+   | Periodically       | Track performance changes over time, protect against regressions |

This means you'll initially work on your dev dataset - an iterative process where you continue to think of the hardest challenges your app could reasonably perform.

As your app gets better, you'll notice that it consistently performs well on certain evals in the dev dataset. You can then move those evals into the CI or regression data set.

### How do you build a good dataset?

We need our data set to represent how our app will be used. The closer the match, the more reliable our evals will be.

Since we're building a demo, our app is a lot less constrained than it would be in the real world.

If we were building for a company, we would also have access to experts who could collaborate with us to build our evals. This helps raise the difficulty of the evals, providing a better signal of the app's quality.

For this demo, I suggest building evals around a topic you know well, and you are well-placed to verify. For me, I'm going to optimize around tracking Arsenal football club's transfer activity. This means I can easily verify the accuracy of the data, and come up with creative evals.

### What about synthetic data?

Synthetic data is a technique where you use an LLM to generate evaluations. This is very useful for certain use cases, such as classifications.

However, for us it's less useful. One of our key metrics is to make sure our data is up to date and fresh. LLM training data has a cut-off date, so synthetic data generated by the LLM won't have the most recent information.

### What makes an eval difficult?

We don't just want to ask our system simple questions. We want to challenge it with difficult, nuanced queries that test its abilities.

One of the most effective ways to test an LLM's capabilities is through **multi-hop reasoning** questions. These are questions that require the model to:

- Break down a complex query into multiple steps
- Find and combine information from different sources
- Make logical connections between pieces of information

For example, in our football transfer scenario, a simple question might be:

> "Who did Arsenal sign in January 2025?"

A multi-hop question would be more challenging:

> "Which Arsenal player from the 2024-25 season has the most international caps, and how does that compare to the player with the most caps in the 2023-24 season?"

This question requires the model to:

1. Find the current Arsenal squad
2. Look up each player's international caps
3. Find the previous season's squad
4. Look up those players' caps
5. Compare the numbers and identify the differences

Multi-hop questions are particularly valuable because they:

- Test the model's ability to maintain context across multiple steps
- Reveal whether the model can accurately combine information from different sources
- Help identify if the model is making assumptions or hallucinating connections
- Provide a more realistic test of how the model will perform in production, where users often ask complex, multi-part questions

## Steps To Complete

- This is a rare case where we won't be using an LLM to complete this exercise.
- Consider what sort of topic you are an expert in, or know a little more than average about.
- Write down three to five basic questions that require recent knowledge in this topic.
- Pass them to the data property in your Evalite eval.
- Run your evals using Evalite to get a sense of how difficult they are.
- Write down three to five multi-hop questions that also require recent knowledge.
- Run your evals again. Continue to iterate until you have a very difficult dataset.
